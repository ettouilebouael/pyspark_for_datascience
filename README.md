# **PySpark pour la Data Science**  

Bienvenue dans le repository du cours **PySpark pour la Data Science**. Ce cours explore les concepts fondamentaux et avancés de PySpark à travers des exemples pratiques et des cas d'utilisation liés à la data science.  

## 📋 **Table des matières**  

- [À propos du cours](#-à-propos-du-cours)  
- [Prérequis](#-prérequis)
- [Organisation](#-organisation)
- [Structure du repository](#-structure-du-repository)
- [Recommandations de lecture](#-recommandations-de-lecture)
- [Contact](#-contact)

## 🎯 **À propos du cours**  

Ce cours a été conçu pour :  
- Introduire les fondamentaux de PySpark, y compris son architecture, ses principes du calcul distribué, ainsi que la manipulation des RDDs et DataFrames.
- Explorer des applications de PySpark dans la data science : nettoyage de données, analyses exploratoires, feature engineering et modélisation.  
- Introduire des concepts avancés tels que les fonctions fenêtre, les optimisations de code, et le machine learning avec MLlib.  

## 💻 **Prérequis**  

Avant de commencer, assurez-vous de disposer des éléments suivants : 

- **Compétences non-techniques** :
  - Une forte motivation et un goût prononcé pour le challenge.
  - Capacité à analyser des problèmes de manière logique et structurée.
  - Curiosité intellectuelle et envie d’explorer de nouveaux concepts.

- **Compétences techniques** :
  - Niveau débutant-intermédiare en Python et ses bibliothèques associées : Pandas et Scikit-learn
  - Niveau débutant-intermédiare en SQL
  - Compréhension des algorithmes de machine learning, notamment la régression linéaire et les modèles basés sur les arbres de décision.
    
- **Environnement de travail** :
Vous pouvez choisir parmi les options suivantes :
  - En local sur votre machine :
    - Python 3.8 ou une version ultérieure.
    - Apache Spark 3.x.
    - Un éditeur comme Jupyter Notebook ou un IDE tel que VS Code.
  - Sur Google Colab
  - Sur Databricks

## 📅 **Organisation**  

- TD1 (2H) : Introduction à Apache Spark et compréhension de ses principes.
- TD2 (2H) : Développement de processus d'analyse de données et de Feature Engineering avec Spark SQL.
- TD3 (2H) : Développement avancé de processus d'analyse de données et de Feature Engineering avec Spark SQL.
- TD4 (2H) : Entraînement distribué d'algorithmes de Machine Learning avec Spark MLlib.
- TD5 (2H) : Évaluation + Feedback sur les livrables

## 📂 **Structure du repository**
    .
    pyspark-data-science/
    ├── data/           # Données utilisées dans les cours et exercices
    ├── cours/          # Slides ou Notebooks Jupyter pour les sessions de cours  
    ├── td_énoncés/     # Exercices pratiques
    ├── td_corrigés/    # Correction des exercices pratiques
    ├── dm/             # Solutions des devoirs maisons
    └── README.md       # Ce fichier

## 📚 **Recommandations de lecture**  

Voici quelques ressources utiles :  

- **Documentation officielle de PySpark** : [Lien](https://spark.apache.org/docs/latest/api/python/index.html)  
- **Principes de base de PySpark (Databricks)** : [Lien](https://learn.microsoft.com/fr-fr/azure/databricks/pyspark/basics)  
- **Feature Engineering pour la prévision de séries temporelles (Medium)** :  [Lien](https://medium.com/@soyoungluna/tutorial-feature-engineering-for-weekly-time-series-forecasting-in-pyspark-b207c41869f4)  
- **Regression with gradient-boosted trees and MLlib pipelines (Databricks)** : [Lien](https://docs.databricks.com/en/_extras/notebooks/source/gbt-regression.html)

## 💡 **Contact**
Pour toute question ou suggestion, contactez-moi à ouael@mailbox.org ou ouvrez une issue sur ce repository.


 
