# **PySpark pour la Data Science**  

Bienvenue dans le repository officiel du cours **PySpark pour la Data Science**. Ce cours explore les concepts fondamentaux et avancÃ©s de PySpark Ã  travers des exemples pratiques et des cas d'utilisation liÃ©s Ã  la data science.  

## ğŸ“‹ **Table des matiÃ¨res**  

- [Ã€ propos du cours](#-Ã -propos-du-cours)  
- [PrÃ©requis](#-prÃ©requis)
- [Organisation](#-organisation)
- [Structure du repository](#-structure-du-repository)
- [Recommandations de lecture](#-recommandations-de-lecture)
- [Contact](#-contact)

## ğŸ¯ **Ã€ propos du cours**  

Ce cours a Ã©tÃ© conÃ§u pour :  
- Enseigner les bases de PySpark, y compris la manipulation de RDDs et DataFrames.  
- Explorer des applications de PySpark dans la data science : nettoyage de donnÃ©es, analyses exploratoires et modÃ©lisation.  
- Introduire des concepts avancÃ©s tels que les fonctions fenÃªtre, les optimisations, et le machine learning avec MLlib.  

## ğŸ’» **PrÃ©requis**  

Avant de commencer, assurez-vous de disposer des Ã©lÃ©ments suivants : 

- **CompÃ©tences non-techniques** :
  - Une forte motivation et un goÃ»t prononcÃ© de challenge.
  - CapacitÃ© Ã  analyser des problÃ¨mes de maniÃ¨re logique et structurÃ©e.
  - CuriositÃ© intellectuelle et envie dâ€™explorer de nouveaux concepts.

- **CompÃ©tences techniques** :
  - MaÃ®trise de Python et des bibliothÃ¨ques associÃ©es : Pandas, Scikit-learn
  - MaÃ®trise de SQL
  - ComprÃ©hension des algorithmes de machine learning, notamment la rÃ©gression linÃ©aire et les modÃ¨les basÃ©s sur les arbres de dÃ©cision.
    
- **Environnement de travail** :
Vous pouvez choisir parmi les options suivantes pour configurer votre environnement :
- En local sur votre machine :
  - Python 3.8 ou une version ultÃ©rieure.
  - Apache Spark 3.x.
  - Un Ã©diteur comme Jupyter Notebook ou un IDE tel que VS Code.
- Sur Google Colab
- Sur Databricks

## ğŸ“… **Organisation**  

- TD1 (2H) : Introduction Ã  Apache Spark et comprÃ©hension de ses principes.
- TD2 (2H) : DÃ©veloppement de processus d'analyse de donnÃ©es et de Feature Engineering avec Spark SQL.
- TD3 (2H) : DÃ©veloppement avancÃ© de processus d'analyse de donnÃ©es et de Feature Engineering avec Spark SQL.
- TD4 (2H) : EntraÃ®nement distribuÃ© d'algorithmes de Machine Learning avec Spark MLlib.
- TD5 (2H) : Ã‰valuation + Feedback sur les livrables

## ğŸ“‚ **Structure du repository**
    .
    pyspark-data-science/
    â”œâ”€â”€ data/           # DonnÃ©es utilisÃ©es dans les exercices
    â”œâ”€â”€ cours/          # Slides ou Notebooks Jupyter pour les sessions de cours  
    â”œâ”€â”€ td/             # Exercices pratiques
    â”œâ”€â”€ td_corrigÃ©s/    # Correction des exercices pratiques
    â”œâ”€â”€ dm/             # Solutions des devoirs maisons
    â””â”€â”€ README.md       # Ce fichier

## ğŸ“š **Recommandations de lecture**  

Voici quelques ressources utiles :  

- **ğŸ“– Documentation officielle de PySpark** : [Lien](https://spark.apache.org/docs/latest/api/python/index.html)  
- **ğŸ” Principes de base de PySpark (Databricks)** : [Lien](https://learn.microsoft.com/fr-fr/azure/databricks/pyspark/basics)  
- **ğŸ’¡ Feature Engineering pour la prÃ©vision de sÃ©ries temporelles (Medium)** :  [Lien](https://medium.com/@soyoungluna/tutorial-feature-engineering-for-weekly-time-series-forecasting-in-pyspark-b207c41869f4)  
- **âš™ï¸ CrÃ©er un modÃ¨le Machine Learning avec Apache Spark MLlib (Microsoft)** : [Lien](https://learn.microsoft.com/fr-fr/fabric/data-science/fabric-sparkml-tutorial)  

## ğŸ’¡ **Contact**
Pour toute question ou suggestion, contactez-moi Ã  ouael@mailbox.org ou ouvrez une issue sur ce repository.


 
