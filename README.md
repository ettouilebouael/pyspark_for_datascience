# **PySpark pour la Data Science**  

Bienvenue dans le repository officiel du cours **PySpark pour la Data Science**. Ce cours explore les concepts fondamentaux et avancés de PySpark à travers des exemples pratiques et des cas d'utilisation liés à la data science.  

## 📋 **Table des matières**  

- [À propos du cours](#-à-propos-du-cours)  
- [Prérequis](#-prérequis)
- [Organisation](#-organisation)
- [Structure du repository](#-structure-du-repository)
- [Recommandations de lecture](#-recommandations-de-lecture)
- [Contact](#-contact)

## 🎯 **À propos du cours**  

Ce cours a été conçu pour :  
- Enseigner les bases de PySpark, y compris la manipulation de RDDs et DataFrames.  
- Explorer des applications de PySpark dans la data science : nettoyage de données, analyses exploratoires et modélisation.  
- Introduire des concepts avancés tels que les fonctions fenêtre, les optimisations, et le machine learning avec MLlib.  

## 💻 **Prérequis**  

Avant de commencer, assurez-vous de disposer des éléments suivants : 

- **Compétences non-techniques** :
  - Une forte motivation et un goût prononcé de challenge.
  - Capacité à analyser des problèmes de manière logique et structurée.
  - Curiosité intellectuelle et envie d’explorer de nouveaux concepts.

- **Compétences techniques** :
  - Maîtrise de Python et des bibliothèques associées : Pandas, Scikit-learn
  - Maîtrise de SQL
  - Compréhension des algorithmes de machine learning, notamment la régression linéaire et les modèles basés sur les arbres de décision.
    
- **Environnement de travail** :
Vous pouvez choisir parmi les options suivantes pour configurer votre environnement :
- En local sur votre machine :
  - Python 3.8 ou une version ultérieure.
  - Apache Spark 3.x.
  - Un éditeur comme Jupyter Notebook ou un IDE tel que VS Code.
- Sur Google Colab
- Sur Databricks

## 📅 **Organisation**  

- TD1 (2H) : Introduction à Apache Spark et compréhension de ses principes.
- TD2 (2H) : Développement de processus d'analyse de données et de Feature Engineering avec Spark SQL.
- TD3 (2H) : Développement avancé de processus d'analyse de données et de Feature Engineering avec Spark SQL.
- TD4 (2H) : Entraînement distribué d'algorithmes de Machine Learning avec Spark MLlib.
- TD5 (2H) : Évaluation + Feedback sur les livrables

## 📂 **Structure du repository**
    .
    pyspark-data-science/
    ├── data/           # Données utilisées dans les exercices
    ├── cours/          # Slides ou Notebooks Jupyter pour les sessions de cours  
    ├── td/             # Exercices pratiques
    ├── td_corrigés/    # Correction des exercices pratiques
    ├── dm/             # Solutions des devoirs maisons
    └── README.md       # Ce fichier

## 📚 **Recommandations de lecture**  

Voici quelques ressources utiles :  

- **📖 Documentation officielle de PySpark** : [Lien](https://spark.apache.org/docs/latest/api/python/index.html)  
- **🔍 Principes de base de PySpark (Databricks)** : [Lien](https://learn.microsoft.com/fr-fr/azure/databricks/pyspark/basics)  
- **💡 Feature Engineering pour la prévision de séries temporelles (Medium)** :  [Lien](https://medium.com/@soyoungluna/tutorial-feature-engineering-for-weekly-time-series-forecasting-in-pyspark-b207c41869f4)  
- **⚙️ Créer un modèle Machine Learning avec Apache Spark MLlib (Microsoft)** : [Lien](https://learn.microsoft.com/fr-fr/fabric/data-science/fabric-sparkml-tutorial)  

## 💡 **Contact**
Pour toute question ou suggestion, contactez-moi à ouael@mailbox.org ou ouvrez une issue sur ce repository.


 
