# **PySpark pour la Data Science**  

Bienvenue dans le repository officiel du cours **PySpark pour la Data Science**. Ce cours explore les concepts fondamentaux et avancÃ©s de PySpark Ã  travers des exemples pratiques et des cas d'utilisation liÃ©s Ã  la data science.  

## ğŸ“‹ **Table des matiÃ¨res**  

- [Ã€ propos du cours](#-Ã -propos-du-cours)  
- [PrÃ©requis](#-prÃ©requis)  
- [Organisation](#-Organisation)  
- [Structure du repository](#-structure-du-repository)
- [Recommendations de lecture](#-recommendation-de-lecture)
- [Contact](#-Contact)

Contact
## ğŸ¯ **Ã€ propos du cours**  

Ce cours a Ã©tÃ© conÃ§u pour :  
- Enseigner les bases de PySpark, y compris la manipulation de RDDs et DataFrames.  
- Explorer des applications de PySpark dans la data science : nettoyage de donnÃ©es, analyses exploratoires et modÃ©lisation.  
- Introduire des concepts avancÃ©s tels que les fonctions fenÃªtre, les optimisations, et le machine learning avec MLlib.  

## ğŸ’» **PrÃ©requis**  

Avant de commencer, assurez-vous de disposer des Ã©lÃ©ments suivants : 

- **CompÃ©tences non-techniques** :
  - Une forte motivation et un goÃ»t prononcÃ© de challenge.
  - CapacitÃ© Ã  analyser des problÃ¨mes de maniÃ¨re logique et structurÃ©e.
  - CuriositÃ© intellectuelle et envie dâ€™explorer de nouveaux concepts.

- **CompÃ©tences techniques** :
  - MaÃ®trise de Python et des bibliothÃ¨ques associÃ©es : Pandas, Scikit-learn
  - MaÃ®trise de SQL
  - ComprÃ©hension des algorithmes de machine learning, notamment la rÃ©gression linÃ©aire et les modÃ¨les basÃ©s sur les arbres de dÃ©cision.
    
- **Environnement de travail** :
Vous pouvez choisir parmi les options suivantes pour configurer votre environnement :
- En local sur votre machine :
  - Python 3.8 ou une version ultÃ©rieure.
  - Apache Spark 3.x.
  - Un Ã©diteur comme Jupyter Notebook ou un IDE tel que VS Code.
- Sur Google Colab
- Sur Databricks

## **Organisation** 
- TD1 (2H) : Introduction Ã  Apache Spark et comprÃ©hension de ses principes.
- TD2 (2H) : DÃ©veloppement de processus d'analyse de donnÃ©es et de Feature Engineering avec Spark SQL.
- TD3 (2H) : DÃ©veloppement avancÃ© de processus d'analyse de donnÃ©es et de Feature Engineering avec Spark SQL.
- TD4 (2H) : EntraÃ®nement distribuÃ© d'algorithmes de Machine Learning avec Spark MLlib.
- TD5 (2H) : Ã‰valuation + Feedback sur les livrables

## ğŸ“‚**Structure du repository**
pyspark-data-science/
â”œâ”€â”€ data/           # DonnÃ©es utilisÃ©es dans les exercices/
â”œâ”€â”€ cours/          # Slides ou Notebooks Jupyter pour les sessions de cours  
â”œâ”€â”€ td/             # Exercies pratiques
â”œâ”€â”€ td_corrigÃ©s/    # Correctoin des exercices pratiques
â”œâ”€â”€ dm/             # Solutions des devoirs maisons
â””â”€â”€ README.md           # Ce fichier  

## **Recommendation de lecture**



## ğŸ’¡ **Contact**
Pour toute question ou suggestion, contactez-moi Ã  ouael@mailbox.org ou ouvrez une issue sur ce repository.


 
